{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity model\n",
    "\n",
    "In this notebook I computed the cosine similarity of the animes sinopses, modeling the features using the ti-idp importance model from the scikit-learn package. The spacy library is used to perform nlp computations: stopwords, tokenization and lemmatization.\n",
    "\n",
    "First we start importing the important packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import spacy\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas() # to show the progressbar in pandas computations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the synopses information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('archive/anime_with_synopsis.csv', index_col='MAL_ID', dtype=str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files are to big to host in this github repository, they are avalilable in https://www.kaggle.com/datasets/hernan4444/anime-recommendation-database-2020\n",
    "\n",
    "The NLP model is provided by spacy, here we import the en_core_web_lg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sypnopsis = df.sypnopsis.dropna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the nlp models for each synopse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16206/16206 [02:59<00:00, 90.46it/s] \n"
     ]
    }
   ],
   "source": [
    "docs = sypnopsis.progress_apply(nlp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function which performs the tokenization and lemmatization from the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    \n",
    "def get_lemmas(doc, stop_words=en_stop_words):\n",
    "    return ' '.join([\n",
    "        token.lemma_\n",
    "        for token in doc\n",
    "        if token.lemma_ not in stop_words\n",
    "        and token.lemma_.isalpha()\n",
    "    ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying it we get the lemmas from each sysnopse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = docs.apply(get_lemmas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Tf-idf model, restricting that each token must appears in at least 100 synopses and at most in 90% of the synopses. This also uses a ngram range from 1 to 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=100, max_df=0.9, ngram_range=(1, 10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_data = tfidf.fit_transform(lemmas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = cosine_similarity(vec_data)\n",
    "cos_sim = pd.DataFrame(cos_sim, index=lemmas.index,\n",
    "                       columns=lemmas.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_idx = cos_sim.sum() > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = cos_sim.loc[valid_idx, valid_idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the recommendation we do not store all the similarities, we need only the top most similar for each one. First we count how many represents the 0.01% most similar for each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = {\n",
    "    idx: row[(row > np.percentile(row, 99.9))&(row < 1)].size\n",
    "    for idx, row in cos_sim.iterrows()\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we take the median value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = np.median([*sizes.values()]).astype(int)\n",
    "size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which means that 16 animes represents the 0.01% most similar to other, at median. So we get the 16 most similar for each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = {\n",
    "    idx: row[~np.isclose(row, 1)].nlargest(size)\n",
    "    for idx, row in cos_sim.iterrows()\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the id info and the weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_id = pd.DataFrame({\n",
    "    key: value.index.values\n",
    "    for key, value in similarities.items()\n",
    "}).T\n",
    "\n",
    "anime_weight = pd.DataFrame({\n",
    "    key: value.values / value.values.sum()\n",
    "    for key, value in similarities.items()\n",
    "}).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pd.concat((anime_id, anime_weight),\n",
    "                    keys=('MAL_ID', 'WEIGHT'),\n",
    "                    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.to_csv('models/weights.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the animes names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_csv('archive/anime.csv', index_col='MAL_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown = info['English name'] == 'Unknown'\n",
    "info.loc[unknown, 'English name'] = info.loc[unknown, 'Name']\n",
    "names = info.loc[weights.index, 'English name'].to_frame()\n",
    "names = names.reset_index()\n",
    "names.columns = 'MAL_ID', 'Name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names.to_csv('data/anime_list.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cab4a9a02ba9d224942f1ffabdf6d58e87b00597d3fa1bb4fb6f453c6be8c97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
